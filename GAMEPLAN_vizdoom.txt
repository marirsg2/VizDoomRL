
==================================

For SML paper analysis

0) Separate code into memory model, and your part to be clear to teacher about what parts were taken.

1) Vary Epsilon, and exploration time AGAINST the number of actions (8 vs 3) and see how long the optimal policy takes

2) Talk about the importance of variance in tracking progress. Because some cases give very high value (monster in middle)
so the average can be misleading. Also training variance is a bad estimate because of the epsilon (exploration rate)
The min is a good indicator for learning too.

3) Talk about adding frames and frame rate influence.

4) Snapping to -300 easily, and then only sometimes recovering. (fr6, k=4, lr 0.00025). Too low learning rate.
or it goes to "center shooting"

5) It seems in general, the problem is GOOD samples dont occur enough, without enough exploration. The larger frame rate
allows us to COVER more space and thus can get GOOD rewards propagated faster !!
Test: vary frame rate. At the frame rate when you get weak but still useful performance, see how tuning the discount factor
probably makes it worse !! A good 2-D graph

5.5) Hmmm... zero frames padding seems to be working better.

6) RELATED to 5, a weak learning rate makes it harder to "snap to" and hold onto the learned GOOD scenario.
ADAM maybe good, if it changes MORE for a significantly deviant sample, as good scenarios will be held onto well.

7) If we fed a reward that was radial distance from the monster, that would help ?

8) HA ! the network actually uses the image at the bottom to decide whats happening in DFC, i.e. am I being hit and
turns to attack.

==================================



DOUBLE CHECK if your exploration rate is set right.

@ see the zero frames correction performance. try k=2, currently k=4

hmm... seems like the zero frames is helping. So repeating is hurting. Which kinda makes sense.
So LSTM feeding one at a time should be the best. Time distributed maybe the middle best.




@try it in repeatFrames  I THINK for more frames to be useful, it needs to have a lower frame repeat ??
@ try it in dfc

Still need to do LSTM  (1) Time Distributed. (just change the model, and either (a) store with channel 1 , or reshape to channel 1)
                        (2) Stateful

LATER: Autoencoder.

Hell, we can make a paper out of this. Atleast for Arxiv.

==========================================================
Maybe the state space is not being properly captured, perhaps an autoencoder !! Is there a default autoencoder module? like resnet

@why is kframe CNN only not working ? ensure that the data is being fed in properly
NO SHUFFLE. Random sampling !! of the CNN frames . Size must be large enough to do this?

I STILL think repeating data in the batches may not be a good idea.feed in batch units, and don't repeat.
OR randomly sample 4 sequential frames for the CNN

@test the improved time distribution on the visual model. EVEN THIS, is probably missing some parts.

@apply the improved time distribution to the old stateful model as well !! review the memory management in that part.


Try batch size == 1. if you set it to be 64, then you update weights for the first 64 steps. Could cover 2 episodes.
REMOVE shuffle == false ?




