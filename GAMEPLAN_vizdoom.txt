
==================================

For SML paper analysis

0) Separate code into memory model, and your part to be clear to teacher about what parts were taken.

1) Vary Epsilon, and exploration time AGAINST the number of actions (8 vs 3) and see how long the optimal policy takes

2) Fewer actions (3) and 1 convolution layer works better, trains faster.
    3 actions works better because we do not get unfairly rewarded for shooting and moving. in DFC this is more so the
    case since monsters MAY spawn when rotating.
    1 conv layer is all we need.

3) Frame rate helps explore the state space here. It also makes fine tuned actions harded. There is a sweet spot around 10


4) Snapping to -300 easily, and then only sometimes recovering. (fr6, k=4, lr 0.00025). Too low learning rate.
or it goes to "center shooting"

5) It seems in general, the problem is GOOD samples dont occur enough, without enough exploration. The larger frame rate
allows us to COVER more space and thus can get GOOD rewards propagated faster !!
Test: vary frame rate. At the frame rate when you get weak but still useful performance, see how tuning the discount factor
probably makes it worse !! A good 2-D graph

5.5) Hmmm... zero frames padding seems to be working better. but even that does do low frame repeats
hmm... seems like the zero frames is helping. So repeating is hurting. Which kinda makes sense.
So LSTM feeding one at a time should be the best. Time distributed maybe the middle best.


6) RELATED to 5, a weak learning rate makes it harder to "snap to" and hold onto the learned GOOD scenario.
ADAM maybe good, if it changes MORE for a significantly deviant sample, as good scenarios will be held onto well.

7) If we fed a reward that was radial distance from the monster, that would help ?

8) HA ! the network actually uses the image at the bottom to decide whats happening in DFC, i.e. am I being hit and
turns to attack.

9) Sensitive to monster turning and which direction it does. More exploration steps covers such cases.

10) 3 actions is MUCH faster to learn. Obviously, the network is not spread thin.
        And previously, it was using a shoot action to TURN !! wasteful.
        AND we could do it with one fewer Convolution layer. (Ablation study)
        One fewer conv layer trains faster and works just as well.
            Makes sense. One filter for monster, one for human position. then maybe one for bullets and such.

11) Adding an extra dense NN layer of size 64, made the training slower (as expected) and performance worse.

12) In LSTM, kframes = 1 with time distributed does not work. Makes sense


13) Smaller LR to avoid stuck at local optimum. Maybe larger batch size too.

14) Smaller frame rate (<6 sucks in Basic, but seems ok in DFC ??)NOT ENOUGH evidence. fr6 seems to work best in dfc too.

15) Especially in LSTM model, it can get stuck in stand and shoot. If low k-frames, then need many steps to explore other options.
Lucky rewards make it more likely to continue that. ? WHy? is no feature processing being done ? what happens to the network ?
Is the reward drop too SHARP that trying anything else is not worth it ? Better to stand and SHOOT in every situation.

16) IS IT data scaling ?? min max scaler on the rewards may help. for DTC, will it avoid the dying network ??

17) A.E. based controller ?

18) Have 3 SEPARATE networks for 3 actions !!. I think the drops will be less. The optimal policy will be used in the succeeding states. Assumption holds

19) Does constant epsilon (exploration) have less variation.

20) WHAT is the reward scale !! the loss function is MSE, but the values could be huge !! what about scaling the values !!
the max score is 26. So divide by 26.

==================================
BASIC results LSTM
BEST: Basic_LSTM_smaller_k2_1k_20ep_lr0006 with zero frames , seems to work best
Basic_LSTM_smaller__fr6_k2_1k_20_ep_lr0006 - Does BETTER? too, with fr6!! which regular does not ?

DFC results LSTM
TOP: (2) DFC_LSTM_smaller_fr10_k2_1k_20_ep_lr0003
(1) DFC_LSTM_smaller_fr6_k3_1k_20_ep_lr0003 - BETTER



==================================

DOUBLE CHECK if your exploration rate is set right.


SEE results of DFC, basic inf for smaller and larger NN architecutres.

RESCALE THE REWARD !! killing monster is +1000, -ve is scaled down by 10.

ABLATION study

Dropout and monster invariance.

SHOULD BE EASIER than you think
1) Have a training network and a target network. (cannot use one lstm) the weights and states ARE CONNECTED.
!!! IMPORTANT 1.5) DO NOT save full episodes, JUST run the target network through the states s2 as needed
2) Update the training network based on the target network


==========================================================


Tensor flow GPU version.

IF ITS SO SENSITIVE TO MONSTER TURNING, weaken the network.
LEARNING RATE of 0.006 seems to work best ??


LATER: Autoencoder.
Hell, we can make a paper out of all these cases. Atleast for Arxiv. Atleast for Andy's thesis.
Dense, CNN, CNN + kframes, CNN + LSTM (time distributed), CNN + LSTM (continuous), Autoencoder + Dense/LSTM+Dense.

==========================================================

Maybe the state space is not being properly captured, perhaps an autoencoder !! Is there a default autoencoder module? like resnet

==========================================================



