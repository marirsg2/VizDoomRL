
==================================

For SML paper analysis

0) Separate code into memory model, and your part to be clear to teacher about what parts were taken.

1) Vary Epsilon, and exploration time AGAINST the number of actions (8 vs 3) and see how long the optimal policy takes

2) Fewer actions (3) and 1 convolution layer works better, trains faster.
    3 actions works better because we do not get unfairly rewarded for shooting and moving. in DFC this is more so the
    case since monsters MAY spawn when rotating.
    1 conv layer is all we need.

3) Frame rate helps explore the state space here. It also makes fine tuned actions harded. There is a sweet spot around 10


4) Snapping to -300 easily, and then only sometimes recovering. (fr6, k=4, lr 0.00025). Too low learning rate.
or it goes to "center shooting"

5) It seems in general, the problem is GOOD samples dont occur enough, without enough exploration. The larger frame rate
allows us to COVER more space and thus can get GOOD rewards propagated faster !!
Test: vary frame rate. At the frame rate when you get weak but still useful performance, see how tuning the discount factor
probably makes it worse !! A good 2-D graph

5.5) Hmmm... zero frames padding seems to be working better. but even that does do low frame repeats
hmm... seems like the zero frames is helping. So repeating is hurting. Which kinda makes sense.
So LSTM feeding one at a time should be the best. Time distributed maybe the middle best.


6) RELATED to 5, a weak learning rate makes it harder to "snap to" and hold onto the learned GOOD scenario.
ADAM maybe good, if it changes MORE for a significantly deviant sample, as good scenarios will be held onto well.

7) If we fed a reward that was radial distance from the monster, that would help ?

8) HA ! the network actually uses the image at the bottom to decide whats happening in DFC, i.e. am I being hit and
turns to attack.

9) Sensitive to monster turning and which direction it does. More exploration steps covers such cases.

10) 3 actions is MUCH faster to learn. Obviously, the network is not spread thin.
        And previously, it was using a shoot action to TURN !! wasteful.
        AND we could do it with one fewer Convolution layer. (Ablation study)
        One fewer conv layer trains faster and works just as well.
            Makes sense. One filter for monster, one for human position. then maybe one for bullets and such.

11) Adding an extra dense NN layer of size 64, made the training slower (as expected) and performance worse.
==================================



DOUBLE CHECK if your exploration rate is set right.

Try frame rate of 8 or 9
With smaller NN, and frame rate of 10. See how k=2, 4,6 affects the DFC performance.


Reward shaping. Show the power of giving the right signal. in basic and DFC. Scott, Josh, Ben


@try it in repeatFrames  I THINK for more frames to be useful, it needs to have a lower frame repeat ??
@ try it in dfc

Still need to do LSTM  (1) Time Distributed. (just change the model, and either (a) store with channel 1 , or reshape to channel 1)
                        (2) Stateful

LATER: Autoencoder.

Hell, we can make a paper out of this. Atleast for Arxiv.

==========================================================
Maybe the state space is not being properly captured, perhaps an autoencoder !! Is there a default autoencoder module? like resnet

@why is kframe CNN only not working ? ensure that the data is being fed in properly
NO SHUFFLE. Random sampling !! of the CNN frames . Size must be large enough to do this?

I STILL think repeating data in the batches may not be a good idea.feed in batch units, and don't repeat.
OR randomly sample 4 sequential frames for the CNN

@test the improved time distribution on the visual model. EVEN THIS, is probably missing some parts.

@apply the improved time distribution to the old stateful model as well !! review the memory management in that part.


Try batch size == 1. if you set it to be 64, then you update weights for the first 64 steps. Could cover 2 episodes.
REMOVE shuffle == false ?




